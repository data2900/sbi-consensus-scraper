# sbi_scraper.py
# Selenium(GUI)ã§ãƒ­ã‚°ã‚¤ãƒ³â†’äººæ‰‹ã§ãƒ‡ãƒã‚¤ã‚¹èªè¨¼â†’Enterã§ç¶šè¡Œï¼†Seleniumè‡ªå‹•çµ‚äº†â†’Playwrightã§ä¸å¯§ï¼†é«˜é€Ÿåé›†
# - ãƒ–ãƒ©ã‚¦ã‚¶ã¯å¿…ãšç«‹ã¡ä¸Šã’ï¼ˆSelenium: headlessä¸ä½¿ç”¨, detach=Falseï¼‰
# - ãƒ­ã‚°ã‚¤ãƒ³ã¯è‡ªå‹•å…¥åŠ›ï¼†é€ä¿¡ã€ãã®å¾Œã¯äººæ‰‹ã§ãƒ‡ãƒã‚¤ã‚¹èªè¨¼ï¼ˆãƒ¯ãƒ³ã‚¿ã‚¤ãƒ ç­‰ï¼‰
# - èªè¨¼å®Œäº†å¾Œã« Enter æŠ¼ä¸‹ â†’ Seleniumã¯è‡ªå‹•ã§é–‰ã˜ã‚‹ â†’ Playwrightã§åé›†é–‹å§‹
# - å››å­£å ±ã‚¿ãƒ–ãŒã‚ã‚Œã°æ¯å›ã‚¯ãƒªãƒƒã‚¯ã—ã¦é–‹ã
# - ç¤¼å„€: å°ä¸¦åˆ—(æ—¢å®š2) + å…¨ä½“QPSåˆ¶å¾¡(æ—¢å®š0.7) + ç”»åƒ/ãƒ•ã‚©ãƒ³ãƒˆé®æ–­ + ãƒªãƒˆãƒ©ã‚¤ + WAL + ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆ
# - 1æœ¬ã§ all / missing å¯¾å¿œ

import os
import sys
import time
import random
import sqlite3
import argparse
import datetime
from typing import Dict, Any, List, Tuple, Optional
from contextlib import asynccontextmanager

# --- Selenium (GUI) for login ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- Playwright for fast scraping ---
import asyncio
from playwright.async_api import async_playwright, TimeoutError as PwTimeout

# ====== èªè¨¼ï¼ˆç›´æ›¸ãï¼šå–ã‚Šæ‰±ã„ã«æ³¨æ„ï¼‰======
USER_ID = "å€‹åˆ¥ã®ID"
PASSWORD = "å€‹åˆ¥ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰"

# ====== å®šæ•° ======
DB_PATH = "/market_data.db"

DEFAULT_CONCURRENCY = 2      # å°ä¸¦åˆ—ï¼ˆ2ã€œ3æ¨å¥¨ï¼‰
DEFAULT_QPS = 0.7            # å…¨ä½“QPSï¼ˆ0.6ã€œ0.9æ¨å¥¨ï¼‰
DEFAULT_BATCH = 100          # DBã‚³ãƒŸãƒƒãƒˆé–“éš”
NAV_TIMEOUT_MS = 25000       # Playwright ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³/å¾…æ©Ÿ
SEL_NAV_TIMEOUT = 25         # Selenium ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ(ç§’)
RETRIES = 3                  # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤
BASE_DELAY = 0.8             # ãƒãƒƒã‚¯ã‚ªãƒ•åˆæœŸ
DEFAULT_LOGIN_WAIT = 60      # è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³å¾Œã€èªè¨¼ã®ç›®å®‰ç§’ï¼ˆä»»æ„ï¼‰

LOGIN_URL = ("https://www.sbisec.co.jp/login")

# å››å­£å ±ã‚¿ãƒ–ï¼ˆã‚ã‚Œã°é–‹ãï¼‰
XPATH_SHIKIHO_TAB = '//*[@id="clmSubArea"]/div[2]/div/div[1]/ul/li[2]/a'

# å–å¾—XPathsï¼ˆSBI å››å­£å ±ã‚¨ãƒªã‚¢ï¼‰
XPATH_MAP: Dict[str, str] = {
    "sales_growth":      '//*[@id="clmMainArea"]/div[12]/table/tbody/tr[1]/td[2]/p/span',
    "op_profit_growth":  '//*[@id="clmMainArea"]/div[12]/table/tbody/tr[2]/td[2]/p/span',
    "op_margin":         '//*[@id="clmMainArea"]/div[12]/table/tbody/tr[3]/td[2]/p',
    "roe":               '//*[@id="clmMainArea"]/div[12]/table/tbody/tr[4]/td[2]/p',
    "roa":               '//*[@id="clmMainArea"]/div[12]/table/tbody/tr[5]/td[2]/p',
    "equity_ratio":      '//*[@id="clmMainArea"]/div[12]/table/tbody/tr[6]/td[2]/p',
    "dividend_payout":   '//*[@id="clmMainArea"]/div[12]/table/tbody/tr[7]/td[2]/p',
}

# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def pct(s: str) -> str:
    s = (s or "").strip()
    if not s:
        return "N/A"
    return s if s.endswith("%") else (s + "%")

def polite_sleep_for_qps(qps: float):
    min_interval = 1.0 / max(qps, 0.01)
    time.sleep(min_interval * (1.0 + random.uniform(-0.15, 0.15)))

# ========= DB =========
def ensure_tables(conn: sqlite3.Connection):
    cur = conn.cursor()
    cur.execute("PRAGMA journal_mode=WAL;")
    cur.execute("PRAGMA synchronous=NORMAL;")
    conn.commit()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS sbi_reports (
            target_date TEXT,
            code TEXT,
            sales_growth TEXT,
            op_profit_growth TEXT,
            op_margin TEXT,
            roe TEXT,
            roa TEXT,
            equity_ratio TEXT,
            dividend_payout TEXT,
            PRIMARY KEY (target_date, code)
        )
    """)
    conn.commit()

def resolve_target_date(conn: sqlite3.Connection, explicit: Optional[str]) -> Optional[str]:
    if explicit:
        datetime.datetime.strptime(explicit, "%Y%m%d")
        return explicit
    cur = conn.cursor()
    cur.execute("SELECT MAX(target_date) FROM consensus_url")
    row = cur.fetchone()
    td = row[0] if row else None
    if td:
        datetime.datetime.strptime(td, "%Y%m%d")
        return td
    return None

def load_targets(conn: sqlite3.Connection, target_date: str, mode: str) -> List[Tuple[str, str]]:
    cur = conn.cursor()
    if mode == "all":
        cur.execute("SELECT code, sbiurl FROM consensus_url WHERE target_date = ?", (target_date,))
        return [(c, u) for c, u in cur.fetchall() if u]
    # missing: nikkei ã‚’æ¯é›†åˆã€sbi_reports æœªä¿å­˜ã®ã¿
    cur.execute("""
        SELECT code FROM nikkei_reports WHERE target_date = ?
        EXCEPT
        SELECT code FROM sbi_reports WHERE target_date = ?
    """, (target_date, target_date))
    codes = [r[0] for r in cur.fetchall()]
    if not codes:
        return []
    ph = ",".join(["?"] * len(codes))
    cur.execute(f"""
        SELECT code, sbiurl FROM consensus_url
        WHERE target_date = ? AND code IN ({ph})
    """, [target_date] + codes)
    return [(c, u) for c, u in cur.fetchall() if u]

# ========= Selenium (GUI) for login =========
def build_selenium() -> Tuple[webdriver.Chrome, str]:
    opts = Options()
    # GUIå¼·åˆ¶ï¼ˆheadlessä¸ä½¿ç”¨ï¼‰
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_experimental_option("detach", False)  # â† Enterå¾Œã«è‡ªå‹•ã§é–‰ã˜ã‚‰ã‚Œã‚‹ã‚ˆã† False
    # è‡ªå‹•åŒ–ç—•è·¡ã‚’è»½æ¸›ï¼ˆå®Œå…¨ã§ã¯ãªã„ï¼‰
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    # DOMContentLoaded ã§å¾©å¸°
    opts.page_load_strategy = "eager"
    ua = "Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari"
    opts.add_argument(f"user-agent={ua}")
    driver = webdriver.Chrome(service=Service(), options=opts)
    driver.set_page_load_timeout(SEL_NAV_TIMEOUT)
    return driver, ua

def sbi_login_auto(driver: webdriver.Chrome, wait_seconds: int):
    driver.get(LOGIN_URL)
    try:
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, "imedisable"))).clear()
        driver.find_element(By.ID, "imedisable").send_keys(USER_ID)
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, '//*[@id="password_input"]/input'))).clear()
        driver.find_element(By.XPATH, '//*[@id="password_input"]/input').send_keys(PASSWORD)
        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, '//*[@id="login"]/ul/li[1]/p[2]/button'))
        ).click()
        print("ğŸ” ãƒ­ã‚°ã‚¤ãƒ³é€ä¿¡æ¸ˆã¿ã€‚ãƒ‡ãƒã‚¤ã‚¹èªè¨¼ã‚’å®Œäº†ã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        print(f"âŒ ãƒ­ã‚°ã‚¤ãƒ³æ“ä½œå¤±æ•—: {e}")
        return

    if wait_seconds > 0:
        time.sleep(wait_seconds)

def export_cookies_for_playwright(driver: webdriver.Chrome) -> List[Dict[str, Any]]:
    raw = driver.get_cookies()
    cookies: List[Dict[str, Any]] = []
    for c in raw:
        cookies.append({
            "name": c.get("name"),
            "value": c.get("value"),
            "domain": c.get("domain") or ".sbisec.co.jp",
            "path": c.get("path", "/"),
            "expires": c.get("expiry", -1),
            "httpOnly": bool(c.get("httpOnly", False)),
            "secure": bool(c.get("secure", True)),
            "sameSite": "Lax",
        })
    return cookies

# ========= Playwright scraping =========
class TokenBucket:
    def __init__(self, qps: float):
        self.interval = 1.0 / max(qps, 0.0001)
        self._lock = asyncio.Lock()
        self._next = time.monotonic()
    async def acquire(self):
        async with self._lock:
            now = time.monotonic()
            if now < self._next:
                await asyncio.sleep(self._next - now)
            self._next = max(now, self._next) + self.interval

@asynccontextmanager
async def playwright_context(play, user_agent: str, seed_cookies: List[Dict[str, Any]]):
    browser = await play.chromium.launch(
        headless=True,  # åé›†ã¯ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ã§è»½é‡åŒ–ï¼ˆSeleniumå´ã¯GUIã§èµ·å‹•æ¸ˆã¿ï¼‰
        args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"]
    )
    context = await browser.new_context(
        user_agent=user_agent,
        java_script_enabled=True,
        bypass_csp=True,
        viewport={"width": 1366, "height": 768}
    )
    # è»½é‡åŒ–: ç”»åƒ/ãƒ•ã‚©ãƒ³ãƒˆ/ãƒ¡ãƒ‡ã‚£ã‚¢é®æ–­
    async def route_handler(route, request):
        if request.resource_type in ("image", "media", "font"):
            await route.abort()
        else:
            await route.continue_()
    await context.route("**/*", route_handler)

    if seed_cookies:
        try:
            await context.add_cookies(seed_cookies)
        except Exception:
            sbisec = [c for c in seed_cookies if "sbisec" in (c.get("domain") or "")]
            if sbisec:
                await context.add_cookies(sbisec)

    try:
        yield context
    finally:
        await context.close()
        await browser.close()

async def fetch_one_pw(page, code: str, url: str) -> Tuple[str, Dict[str, Any]]:
    await page.goto(url, wait_until="domcontentloaded", timeout=NAV_TIMEOUT_MS)
    # å››å­£å ±ã‚¿ãƒ–ï¼šå­˜åœ¨ã™ã‚Œã°ã‚¯ãƒªãƒƒã‚¯ï¼ˆå¤±æ•—ã¯ç„¡è¦–ï¼‰
    try:
        btn = await page.wait_for_selector(f"xpath={XPATH_SHIKIHO_TAB}", timeout=3000)
        if btn:
            await btn.click()
            await asyncio.sleep(0.5)
    except Exception:
        pass

    data = await page.evaluate(
        """(xps) => {
            const get = (xp) => {
              try { return document.evaluate(xp, document, null, XPathResult.STRING_TYPE, null).stringValue.trim(); }
              catch(e){ return ""; }
            };
            const out = {};
            for (const [k, xp] of Object.entries(xps)) out[k] = get(xp);
            return out;
        }""",
        XPATH_MAP
    )
    for k in list(data.keys()):
        data[k] = pct(data.get(k, ""))
    return code, data

async def worker(ctx, jobs: asyncio.Queue, bucket: TokenBucket, results: asyncio.Queue):
    page = await ctx.new_page()
    try:
        while True:
            item = await jobs.get()
            if item is None:
                break
            code, url = item
            await bucket.acquire()
            delay = BASE_DELAY
            last_err = None
            for attempt in range(RETRIES):
                try:
                    c, d = await fetch_one_pw(page, code, url)
                    await results.put((c, d, None))
                    break
                except (PwTimeout, Exception) as e:
                    last_err = e
                    if attempt < RETRIES - 1:
                        await asyncio.sleep(delay); delay *= 1.8
                    else:
                        await results.put((code, None, last_err))
            jobs.task_done()
    finally:
        await page.close()

async def run_playwright_scrape(targets: List[Tuple[str,str]], ua: str, cookies: List[Dict[str, Any]],
                                target_date: str, qps: float, concurrency: int, batch: int):
    conn = sqlite3.connect(DB_PATH)
    ensure_tables(conn)
    cur = conn.cursor()

    jobs: asyncio.Queue = asyncio.Queue()
    results: asyncio.Queue = asyncio.Queue()
    for t in targets:
        await jobs.put(t)
    total = len(targets)
    done = ok = ng = 0
    buf: List[Tuple] = []
    bucket = TokenBucket(qps)

    async with async_playwright() as play:
        async with playwright_context(play, ua, cookies) as ctx:
            workers = [asyncio.create_task(worker(ctx, jobs, bucket, results))
                       for _ in range(max(1, min(concurrency, 6)))]

            async def stop_workers():
                for _ in workers:
                    await jobs.put(None)

            try:
                while done < total:
                    code, data, err = await results.get()
                    done += 1
                    if err is None and data:
                        row = (
                            target_date, code,
                            data.get("sales_growth",""), data.get("op_profit_growth",""),
                            data.get("op_margin",""), data.get("roe",""), data.get("roa",""),
                            data.get("equity_ratio",""), data.get("dividend_payout","")
                        )
                        buf.append(row); ok += 1
                        if len(buf) >= batch:
                            cur.executemany("""
                                INSERT OR REPLACE INTO sbi_reports (
                                    target_date, code, sales_growth, op_profit_growth,
                                    op_margin, roe, roa, equity_ratio, dividend_payout
                                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, buf)
                            conn.commit(); buf.clear()
                    else:
                        ng += 1
                    if done % 50 == 0 or done == total:
                        print(f"âœ… {done}/{total} / OK:{ok} NG:{ng}")
            finally:
                if buf:
                    cur.executemany("""
                        INSERT OR REPLACE INTO sbi_reports (
                            target_date, code, sales_growth, op_profit_growth,
                            op_margin, roe, roa, equity_ratio, dividend_payout
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, buf)
                    conn.commit()
                await stop_workers()
                await asyncio.gather(*workers, return_exceptions=True)
                conn.close()
                print(f"ğŸ å®Œäº† / OK:{ok} NG:{ng} / å¯¾è±¡:{total}")

# ========= ãƒ¡ã‚¤ãƒ³ï¼ˆSeleniumã§ãƒ­ã‚°ã‚¤ãƒ³â†’Enterã§é–‰ã˜ã‚‹â†’Playwrightã§åé›†ï¼‰=========
def main():
    p = argparse.ArgumentParser()
    p.add_argument("-a", "--target_date", help="YYYYMMDDï¼ˆæœªæŒ‡å®šãªã‚‰ consensus_url ã®æœ€æ–°æ—¥ä»˜ï¼‰")
    p.add_argument("--mode", choices=["all", "missing"], default="missing",
                   help="all: consensus_url å…¨ä»¶ / missing: nikkeiåŸºæº–ã§æœªå–å¾—ã®ã¿")
    p.add_argument("--qps", type=float, default=DEFAULT_QPS, help="å…¨ä½“QPSä¸Šé™ï¼ˆ0.6ã€œ0.9æ¨å¥¨ï¼‰")
    p.add_argument("--concurrency", type=int, default=DEFAULT_CONCURRENCY, help="Playwrightä¸¦åˆ—ï¼ˆ2ã€œ3æ¨å¥¨ï¼‰")
    p.add_argument("--batch", type=int, default=DEFAULT_BATCH, help="DBã‚³ãƒŸãƒƒãƒˆé–“éš”")
    p.add_argument("--login-wait", type=int, default=DEFAULT_LOGIN_WAIT, help="è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³å¾Œã«å¾…ã¤ç§’æ•°ï¼ˆèªè¨¼ç›®å®‰ï¼‰")
    args = p.parse_args()

    conn = sqlite3.connect(DB_PATH)
    ensure_tables(conn)
    target_date = resolve_target_date(conn, args.target_date)
    if not target_date:
        print("âŒ target_date ã‚’æ±ºå®šã§ãã¾ã›ã‚“ï¼ˆ-a YYYYMMDD ã‚’æŒ‡å®šã™ã‚‹ã‹ã€consensus_url ã«ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼‰")
        conn.close(); sys.exit(1)

    targets = load_targets(conn, target_date, args.mode)
    if not targets:
        if args.mode == "missing":
            print(f"âœ… {target_date} æœªå–å¾—ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆmode=missingï¼‰")
        else:
            print(f"âš ï¸ {target_date} å¯¾è±¡URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆmode=allï¼‰")
        conn.close(); return
    conn.close()

    # 1) Selenium(GUI) ã§ãƒ­ã‚°ã‚¤ãƒ³â†’ãƒ‡ãƒã‚¤ã‚¹èªè¨¼ï¼ˆEnterã§ç¶šè¡Œï¼†è‡ªå‹•ã‚¯ãƒ­ãƒ¼ã‚ºï¼‰
    print("ğŸŒ Chrome(GUI) ã‚’èµ·å‹•ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚")
    driver, ua = build_selenium()
    try:
        sbi_login_auto(driver, wait_seconds=args.login_wait)
        input("â¸ èªè¨¼ãŒå®Œäº†ã—ä¼šå“¡ãƒšãƒ¼ã‚¸ãŒé–‹ã‘ã‚‹çŠ¶æ…‹ã«ãªã£ãŸã‚‰ Enter ã‚’æŠ¼ã—ã¦ãã ã•ã„â€¦ ")

        # èªè¨¼å¾Œã® Cookie ã‚’ Playwright ã¸ç§»æ¤
        cookies = export_cookies_for_playwright(driver)
    finally:
        # â† Enterã®å¾Œã¯**å¿…ãš**Seleniumãƒ–ãƒ©ã‚¦ã‚¶ã‚’è‡ªå‹•ã§é–‰ã˜ã‚‹
        try:
            driver.quit()
        except Exception:
            pass

    if not cookies:
        print("âš ï¸ Cookie ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚èªè¨¼ãŒå®Œäº†ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ç¶šè¡Œã¯å¯èƒ½ã§ã™ãŒå¤±æ•—ã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")

    # 2) Playwright ã§ä¸å¯§ã«é«˜é€Ÿå–å¾—ï¼ˆå°ä¸¦åˆ—ï¼‹QPSåˆ¶å¾¡ï¼‰
    print(f"â–¶ å–å¾—é–‹å§‹: mode={args.mode} date={target_date} / concurrency={args.concurrency} qps={args.qps}")
    asyncio.run(run_playwright_scrape(
        targets=targets,
        ua=ua,
        cookies=cookies,
        target_date=target_date,
        qps=args.qps,
        concurrency=args.concurrency,
        batch=args.batch
    ))

if __name__ == "__main__":
    main()
